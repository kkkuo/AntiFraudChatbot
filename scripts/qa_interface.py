from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

def load_qa_chain():
    # æ¨¡å‹åç¨±
    model_id = "ziqingyang/chinese-alpaca-2-7b"

    # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype="auto"
    )

    # å»ºç«‹ text-generation pipeline
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=True)

    # åŒ…è£æˆ LangChain çš„ LLM
    llm = HuggingFacePipeline(pipeline=pipe)

    # å»ºç«‹å‘é‡åµŒå…¥å™¨
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    # è¼‰å…¥ FAISS è³‡æ–™åº«ï¼ˆå‡è¨­ä½ å·²å„²å­˜åœ¨ faiss_index/ï¼‰
    vectorstore = FAISS.load_local("faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)

    # å»ºç«‹ QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )

    return qa_chain

def run_qa(query):
    qa_chain = load_qa_chain()
    result = qa_chain.invoke(query)

    print("ğŸ” å•é¡Œï¼š", query)
    print("\nğŸ“˜ å›ç­”ï¼š", result["result"])
    print("\nğŸ“ ç›¸é—œè³‡æ–™ä¾†æºï¼š")
    for doc in result["source_documents"]:
        print(doc.page_content)
        print("-" * 40)

# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    test_query = "æœ‰äººèªªæˆ‘å¸³æˆ¶ç•°å¸¸ï¼Œè¦æˆ‘æ“ä½œATMï¼Œæˆ‘è©²æ€éº¼è¾¦ï¼Ÿ"
    run_qa(test_query)
